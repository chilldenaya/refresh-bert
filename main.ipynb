{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "sharing-lafayette",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 100000 lines in : 0:00:01.483051\n",
      "processed 200000 lines in : 0:00:01.504671\n",
      "processed 300000 lines in : 0:00:01.557410\n",
      "processed 400000 lines in : 0:00:01.536786\n",
      "processed 500000 lines in : 0:00:01.534796\n",
      "processed 600000 lines in : 0:00:01.540017\n",
      "processed 700000 lines in : 0:00:01.603163\n",
      "processed 800000 lines in : 0:00:01.558970\n",
      "processed 900000 lines in : 0:00:01.567573\n",
      "processed 1000000 lines in : 0:00:01.559157\n",
      "processed 1100000 lines in : 0:00:01.572110\n",
      "processed 1200000 lines in : 0:00:01.581173\n",
      "processed 1300000 lines in : 0:00:01.586955\n",
      "processed 1400000 lines in : 0:00:01.657445\n",
      "processed 1500000 lines in : 0:00:01.557385\n",
      "processed 1600000 lines in : 0:00:01.578946\n",
      "processed 1700000 lines in : 0:00:01.576346\n",
      "processed 1800000 lines in : 0:00:01.578137\n",
      "processed 1900000 lines in : 0:00:01.601763\n",
      "processed 2000000 lines in : 0:00:01.576514\n",
      "processed 2100000 lines in : 0:00:01.597541\n",
      "processed 2200000 lines in : 0:00:01.635089\n",
      "processed 2300000 lines in : 0:00:01.603235\n",
      "processed 2400000 lines in : 0:00:01.592307\n",
      "processed 2500000 lines in : 0:00:01.601646\n",
      "processed 2600000 lines in : 0:00:01.593642\n",
      "processed 2700000 lines in : 0:00:01.602368\n",
      "processed 2800000 lines in : 0:00:01.785162\n",
      "processed 2900000 lines in : 0:00:01.592758\n",
      "processed 3000000 lines in : 0:00:01.599507\n",
      "processed 3100000 lines in : 0:00:01.577773\n",
      "processed 3200000 lines in : 0:00:01.591407\n",
      "processed 3300000 lines in : 0:00:01.599997\n",
      "processed 3400000 lines in : 0:00:01.631716\n",
      "processed 3500000 lines in : 0:00:01.555779\n",
      "processed 3600000 lines in : 0:00:01.526298\n",
      "processed 3700000 lines in : 0:00:01.633035\n",
      "processed 3800000 lines in : 0:00:01.653844\n",
      "processed 3900000 lines in : 0:00:01.624727\n",
      "processed 4000000 lines in : 0:00:01.600270\n",
      "processed 4100000 lines in : 0:00:01.573363\n",
      "processed 4200000 lines in : 0:00:01.556635\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "def create_word2vec_word2idx(vector_file):\n",
    "    word2idx = {\n",
    "        \"_PAD\": 0,\n",
    "        \"_UNK\": 1,\n",
    "    }\n",
    "    idx = 2\n",
    "\n",
    "    with open(vector_file, 'r', encoding='utf-8') as file:\n",
    "        now = datetime.now()\n",
    "        \n",
    "        for line in file:\n",
    "            try:\n",
    "                line = line.strip().split()\n",
    "                word = line[0]\n",
    "\n",
    "                if word not in word2idx:\n",
    "                    word2idx[word] = idx\n",
    "                    idx += 1\n",
    "            except Exception as e:\n",
    "                continue\n",
    "\n",
    "            if ( idx + 1 ) % 100000 == 0:\n",
    "                print(f\"processed {idx+1} lines in : {datetime.now() - now}\")\n",
    "                now = datetime.now()\n",
    "            \n",
    "    return word2idx\n",
    "\n",
    "vector_file = '/home/jupyter-23521027/refresh-bert/data/indo4b/fasttext.4B.id.300.epoch5.uncased.vec'\n",
    "word2idx = create_word2vec_word2idx(vector_file)\n",
    "model = SentenceTransformer('/home/jupyter-23521027/refresh-bert/data/sbert/indobert-large-p1_dense_trained-epoch14', device='cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "passing-replication",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def prepare_data(sent_tokenized):\n",
    "    MAX_DOC_LENGTH = 100\n",
    "    MAX_SENT_EMBEDDING_LENGTH = 250\n",
    "    DATASET_SIZE = 999999\n",
    "    data = {\n",
    "        \"id\": [1],\n",
    "        \"sentences\": [sent_tokenized]\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    file = open(\"/home/jupyter-23521027/refresh-bert/data/preprocessed-input-directory/liputan6.inference.doc\" , 'w')\n",
    "    for i, row in df.iterrows():\n",
    "        file.write(f'liputan6-{str(row[\"id\"])}' + \"\\n\")\n",
    "\n",
    "        sentences = list(row[\"sentences\"][:MAX_DOC_LENGTH])\n",
    "        for sentence in sentences:\n",
    "            for word in sentence.split():\n",
    "                try:\n",
    "                    index = str(word2idx[word.lower()])\n",
    "                except Exception as KeyError:\n",
    "                    index = str(word2idx[\"_UNK\"])\n",
    "                file.write(index + \" \")\n",
    "\n",
    "            file.write(\"\\n\")\n",
    "        file.write(\"\\n\")\n",
    "    file.close()\n",
    "\n",
    "    file_path = \"/home/jupyter-23521027/refresh-bert/data/preprocessed-input-directory/liputan6.inference.sbert\"\n",
    "    file = open(file_path , 'w')\n",
    "    for i, row in df.iterrows():\n",
    "        file.write(f\"liputan6-{str(row['id'])}\\n\")\n",
    "\n",
    "        for sent_txt in row[\"sentences\"][:MAX_DOC_LENGTH]:\n",
    "            sent_embedding = model.encode(sent_txt, convert_to_numpy=True).tolist()[:MAX_SENT_EMBEDDING_LENGTH]\n",
    "\n",
    "            for element in sent_embedding:\n",
    "                file.write(str(element) + \" \")\n",
    "\n",
    "            file.write(\"\\n\")\n",
    "\n",
    "        file.write(\"\\n\")\n",
    "\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cloudy-advice",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Salah satu warga Banyuwangi, Jeni (50) mengaku, beberapa hari tidak bisa memasak karena tidak ada LPG 3 kilogram .', 'Dia terpaksa membeli masakan siap saji untuk makan setiap hari .', '\" Kenapa ini gas ini kok bisa langka seperti ini . Saya sudah tiga hari tidak masak karena tidak ada gas , masak di Banyuwangi terjadi Kelangkaan seperti ini,\" sesal Jeni sembari mengantre mendapatkan gas LPG 3 kilogram pada Senin 24 Juli 2023.', 'Kata Jeni, selain langka, harga gas 3 kilogram di tingkat pangkalan juga naik.', 'Dari yang harga normalnya Rp 18 ribu per tabung, saat ini sudah mencapai Rp 22 ribu hingga Rp 25 ribu per tabung.', '\"Saya tidak mempermasalahkan kenaikan itu, tapi yang penting barangnya ada. Naik tapi barangnya tidak ada ini kan justru menyulitkan kita, karena mau masak tidak bisa, kembali menggunakan kayu juga tidak bisa, siapa yang jualan kayu sekarang,\" tegasnya.', 'Masalah kelangkaan LPG 3 kg ini sampai di telinga Presiden Joko Widodo atau Jokowi.', 'Dia pun kemudian menegaskan lagi bahwa LPG bersubsidi hanya diperuntukkan bagi masyarakat yang kurang mampu.', '\"LPG terutama yang bersubsidi, ini memang diperebutkan di lapangan dan itu hanya untuk yang kurang mampu.', 'Itu yang harus digarisbawahi,\" jelas Jokowi saat melakukan kunjungan kerja ke Pasar Bululawang Kabupaten Malang, Jawa Timur, pada Senin 24 Juli 2023.', 'Jokowi enggan menjelaskan secara detail penyebab kelangkaan gas LPG bersubsidi.', 'Jokowi meminta agar hal tersebut ditanyakan kepada Menteri BUMN Erick Thohir.']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path = \"/home/jupyter-23521027/refresh-bert/data/Liputan6-Filtered-TokenizedSegmented/inference/1.mainbody\"\n",
    "with open(file_path, 'r') as file:\n",
    "    lines = file.readlines()\n",
    "    sent_tokenized = [line.strip() for line in lines]\n",
    "    print(sent_tokenized)\n",
    "\n",
    "prepare_data(sent_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ideal-afghanistan",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
